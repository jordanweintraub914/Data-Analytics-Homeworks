# Grading rubrics

## How we will grade the assignment

- Readme: 35%
	- Does it use 2+ levels of headers (Y/N)
	- Does it use bold and italics (Y/N)
	- Does it use a picture or gif(Y/N)
	- Does it use a list (Y/N)
	- Does it use a table (Y/N)
	- Does it use a code snippet(Y/N)
	- How many grammar or formatting errors are there? (0 or 1, 2 or more)
	- Small bonus: Is the readme especially creative or "beyond expectations" in quality and/or features? (Y/N)
- Python file: 45%
	- When you look at the python file _**in the repo online,**_ can you see the outputs from the code? (Y/N)
	- Are those outputs numbered consecutively from 1-10? (Y/N) , and put no if you can't see the output online. 
	- GRADER: clone this repo to your computer and run the file. Did it run without error? (Y/N)
	- When you run it, does it create "temp.csv"? (Y/N) 
	- How many of the 10 exercises did it solve correctly? (1-10)
- gitignore: 10%
	- The online repo should only have these files: .gitignore, README.md, temp/wine.csv, instructions/instructions.md, instructions/rubric.md, and asgn01exercises.ipynb (Y/N)
	- Did they modify `.gitignore` to include an auxilliary files produced when they worked on the code?
- Reviewer remarks (be nice - the spirit is learning collaboratively!)
	- Add some specific praise or describe something you learned
	- Do you have any constructive criticism?
	- If you think they struggled with part of the homework, share what you know about it that might help them. 
	
## How we will grade the peer review 

1. Accuracy and honor code: 80% 
	- We check 25% of peer reviews at random. Errors in reviews will reduce your peer evaluation grade 
	- If your review is materially wrong, we will audit all of your reviews for the semester and adjust those grades and yours accordingly. 
	- We will also check reviews if your reviewee thinks your review is materially wrong, mean spirited, or if we are told of academic honest violations. 
2. Feedback quality on remarks: 20%