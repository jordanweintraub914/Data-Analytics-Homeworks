{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tight-graduation",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-singer",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from time import sleep\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-government",
   "metadata": {},
   "source": [
    "### Regex Function Defined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gentle-nicaragua",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def NEAR_regex(list_of_words,max_words_between=5,partial=False,cases_matter=False):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_words : list\n",
    "        A list of \"words\", each element is a string\n",
    "        \n",
    "        This program will return a regex that will look for times where word1 \n",
    "        is near word2, or word2 is near word 1.\n",
    "        \n",
    "        It works with multiple words: You can see if words1 is near word2 or\n",
    "        word3. \n",
    "        \n",
    "    max_words_between : int, optional\n",
    "        How many \"words\" are allowed between words in list_of_words. The default\n",
    "        is 5, but you should consider this carefully.\n",
    "        \n",
    "        \"words\" in between are chunks of characters. \"DON don don- don12 2454\" \n",
    "        is 5 words.\n",
    "        \n",
    "        This will not allow matches if the words are separated by a newline \n",
    "        (\"\\n\") character.\n",
    "        \n",
    "    partial : Boolean, optional\n",
    "        If true, will accept longer words than you give. For example, if one \n",
    "        word in your list is \"how\", it will match to \"howdy\". Be careful in \n",
    "        choosing this based on your problem. Partial makes more sense with \n",
    "        longer words. \n",
    "        The default is True.\n",
    "        \n",
    "    cases_matter: Boolean, optional bt IMPORTANT\n",
    "        If True, will return a regex string that will only catch cases where  \n",
    "        words in the string have the same case as given as input to this \n",
    "        function. For example, if one word here is \"Hi\", then the regex \n",
    "        produced by this function will not catch \"hi\".\n",
    "        \n",
    "        If false, will return a regex string that will only work if all letters\n",
    "        in search string are lowercase.\n",
    "        \n",
    "        The default is True.\n",
    "     \n",
    "        \n",
    "    Warning / Feature\n",
    "    -------\n",
    "    This WILL NOT ... (missing documentation!)\n",
    "    \n",
    "        \n",
    "    Unsure about speed\n",
    "    -------\n",
    "    I don't think this is a very \"fast\" function, but it should be robust. \n",
    "  \n",
    "    \n",
    "    Suggested use\n",
    "    -------\n",
    "    a_string_you_have = 'Jack and Jill went up the hill'\n",
    "    \n",
    "    # 1. define words and set up the regex\n",
    "    words = ['jack','hill']                         \n",
    "    rgx = NEAR_regex(words)                       \n",
    "    \n",
    "    # 2. convert the string to lowercase before searching!\n",
    "    a_string_you_have = a_string_you_have.lower()   \n",
    "    \n",
    "    # 3. len+findall+rgx = counts the number of times the word groups are close\n",
    "    count = len(re.findall(rgx,test))              \n",
    "    print(count)                                 \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A string which is a regex that can be used to look for cases where all the \n",
    "    input words are near each other.\n",
    "    '''\n",
    "               \n",
    "    from itertools import permutations\n",
    "    \n",
    "    start = r'(?:\\b' # the r means \"raw\" as in the backslash is just a backslash, not an escape character\n",
    "    \n",
    "    if partial:\n",
    "        gap   = r'[A-Za-z]*\\b(?: +[^ \\n\\r]*){0,' +str(max_words_between)+r'} *\\b'\n",
    "        end   = r'[A-Za-z]*\\b)'\n",
    "    else:\n",
    "        gap   = r'\\b(?: +[^ \\n]*){0,' +str(max_words_between)+r'} *\\b'\n",
    "        end   = r'\\b)'\n",
    "        \n",
    "    regex_list = []\n",
    "    \n",
    "    for permu in list(permutations(list_of_words)):\n",
    "        # catch this permutation: start + word + gap (+ word + gap)... + end\n",
    "        if cases_matter: # case sensitive - what cases the user gives are given back\n",
    "              regex_list.append(start+gap.join(permu)+end)           \n",
    "        else: # the resulting search will only work if all words are lowercase\n",
    "            lowerpermu = [w.lower() for w in permu]\n",
    "            regex_list.append(start+gap.join(lowerpermu)+end)\n",
    "    \n",
    "    return '|'.join(regex_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-account",
   "metadata": {},
   "source": [
    "### Read in the csv located in the inputs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "casual-tribune",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sp500 = pd.read_csv('inputs/sp500_with_url.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-collins",
   "metadata": {},
   "source": [
    "### RISKS\n",
    "\n",
    "supply chain risk version 1\n",
    "- large gap between words (10,000)\n",
    "- result: good results\n",
    "\n",
    "supply chain risk version 2\n",
    "- small gap between words (1,000)\n",
    "- more topic words\n",
    "- result: great results!\n",
    "\n",
    "supply chain risk version 3\n",
    "- large gap between words (10,000)\n",
    "- unhelpful topic and risk words \n",
    "- result: many innacurate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "equipped-rainbow",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 505/505 [01:18<00:00,  6.44it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for index, row in tqdm(sp500.iterrows(),total=len(sp500)): #loops through dataframe\n",
    "    location = 'text_files/' + row['Symbol'] + '.html' #pull downloaded files (wiki firm page)\n",
    "    if not os.path.isfile(location): #if file doesn't have a location, skip over it\n",
    "        continue #if it doesn't have a location, continue to next item in for loop\n",
    "    with open(location, 'r') as file: #open the file \n",
    "        html = file.read() #ready through the file \n",
    "         \n",
    "    lower = BeautifulSoup(html).get_text().lower() #scan through html file read from above \n",
    "    no_punc = re.sub(r'\\W',' ',lower) #letters and num\n",
    "    cleaned = re.sub(r'\\s+',' ',no_punc).strip() #removes spaces\n",
    "\n",
    "    \n",
    "    \n",
    "    #RISK1: SUPPLY CHAIN (1st measure - avg)\n",
    "    supply_chain_risk_words = ['(supply chain|supply|production|materials|capacity|inventory)', # list the topic words\n",
    "                           '(risk|risks|concern|concerns)'] # list of \"risk\" words\n",
    "    supply_chain_risk_rgx = NEAR_regex(supply_chain_risk_words, max_words_between=10000,partial=True) # creates the re pattern\n",
    "    RISK_supply_chain = len(re.findall(supply_chain_risk_rgx,cleaned)) # look for that pattern\n",
    "    sp500.loc[index,'RISK1: supply chain'] = RISK_supply_chain #add risk var to only ONE row\n",
    "    #print(index, RISK_supply_chain, row['url']) #lmk how many matches per symbol (way to self check) (#hits, matched to each row)\n",
    "  \n",
    "    #RISK1: SUPPLY CHAIN (2nd measure - good)\n",
    "    supply_chain_risk_words_2 = ['(supply chain|supply|production|materials|capacity|inventory|product|products)', # list the topic words\n",
    "                           '(risk|risks|bad|cautious|worry|concern|concerns)'] # list of \"risk\" words\n",
    "    supply_chain_risk_rgx_2 = NEAR_regex(supply_chain_risk_words_2, max_words_between=1000) # creates the re pattern\n",
    "    RISK_supply_chain_2 = len(re.findall(supply_chain_risk_rgx_2,cleaned)) # look fir that pattern\n",
    "    #print(RISK_supply_chain)\n",
    "    sp500.loc[index,'RISK1: supply chain (2)'] = RISK_supply_chain_2 #add risk var to only ONE row\n",
    "\n",
    "   \n",
    "    #RISK1: SUPPLY CHAIN (3rd measure - bad)\n",
    "    supply_chain_risk_words_3 = ['(supply chain|supply|production|materials|has|and|it)', # list the topic words\n",
    "                           '(risk|risks|concern|concerns|bad|cautious|worry|found|in|at)'] # list of \"risk\" words\n",
    "    supply_chain_risk_rgx_3 = NEAR_regex(supply_chain_risk_words_3, max_words_between=10000) # creates the re pattern\n",
    "    RISK_supply_chain_3 = len(re.findall(supply_chain_risk_rgx_3,cleaned)) # look fir that pattern\n",
    "    sp500.loc[index,'RISK1: supply chain (3)'] = RISK_supply_chain_3 #add risk var to only ONE row\n",
    "\n",
    "\n",
    "    \n",
    "    #RISK2: LITIGATION \n",
    "    litigation_risk_words = ['(litigation|law|legal|lawsuit|lawsuits|class action|sue|sues|fine|fines|fined|pending)', \n",
    "                           '(risk|risks|concern|concerns)'] \n",
    "    litigation_risk_rgx = NEAR_regex(litigation_risk_words, max_words_between=1000,partial=True) \n",
    "    RISK_litigation = len(re.findall(litigation_risk_rgx,cleaned)) \n",
    "    sp500.loc[index,'RISK2: Litigation'] = RISK_litigation \n",
    "    #print(index, RISK_litigation, row['url']) \n",
    "\n",
    "    \n",
    "    #RISK3: INFLATION\n",
    "    inflation_risk_words = ['(inflation|inflationary|economy|hyperinflation|deflation|inflate|inflated|dollar|USD)', \n",
    "                           '(risk|risks|concern|concerns)'] \n",
    "    inflation_risk_rgx = NEAR_regex(inflation_risk_words, max_words_between=1000, partial=True) \n",
    "    RISK_inflation = len(re.findall(inflation_risk_rgx,cleaned)) \n",
    "    sp500.loc[index,'RISK3: Inflation'] = RISK_inflation \n",
    "    #print(index, RISK_inflation, row['url']) \n",
    "\n",
    "    \n",
    "#sp500.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-viking",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "metric-parking",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>RISK1: supply chain</th>\n",
       "      <th>RISK1: supply chain (2)</th>\n",
       "      <th>RISK1: supply chain (3)</th>\n",
       "      <th>RISK2: Litigation</th>\n",
       "      <th>RISK3: Inflation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.050000e+02</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.731188e+05</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.704365</td>\n",
       "      <td>1.123016</td>\n",
       "      <td>0.670635</td>\n",
       "      <td>0.396825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.502642e+05</td>\n",
       "      <td>0.572710</td>\n",
       "      <td>1.288387</td>\n",
       "      <td>0.423873</td>\n",
       "      <td>1.313128</td>\n",
       "      <td>0.795908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.800000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.375100e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.753200e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.132979e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.792044e+06</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                CIK  RISK1: supply chain  RISK1: supply chain (2)  \\\n",
       "count  5.050000e+02           504.000000               504.000000   \n",
       "mean   7.731188e+05             0.339286                 0.704365   \n",
       "std    5.502642e+05             0.572710                 1.288387   \n",
       "min    1.800000e+03             0.000000                 0.000000   \n",
       "25%    9.375100e+04             0.000000                 0.000000   \n",
       "50%    8.753200e+05             0.000000                 0.000000   \n",
       "75%    1.132979e+06             1.000000                 1.000000   \n",
       "max    1.792044e+06             4.000000                11.000000   \n",
       "\n",
       "       RISK1: supply chain (3)  RISK2: Litigation  RISK3: Inflation  \n",
       "count               504.000000         504.000000        504.000000  \n",
       "mean                  1.123016           0.670635          0.396825  \n",
       "std                   0.423873           1.313128          0.795908  \n",
       "min                   1.000000           0.000000          0.000000  \n",
       "25%                   1.000000           0.000000          0.000000  \n",
       "50%                   1.000000           0.000000          0.000000  \n",
       "75%                   1.000000           1.000000          1.000000  \n",
       "max                   5.000000          10.000000          6.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-agreement",
   "metadata": {},
   "source": [
    "Necessary steps:\n",
    "\n",
    "- read in  data file \n",
    "- merge it with the the dataframe that includes our new risk variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "antique-samuel",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#add 2019 accounting data\n",
    "url='https://github.com/LeDataSciFi/ledatascifi-2021/blob/main/data/2019%20ccm_cleaned.dta?raw=true'\n",
    "\n",
    "acct_2019 = pd.read_stata(url) #reads in the data file \n",
    "\n",
    "#merge with sp500 dataset \n",
    "sp500_with_acct = pd.merge(sp500,acct_2019, left_on='Symbol', right_on='tic', how='left', indicator=True, validate='one_to_one')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-plumbing",
   "metadata": {},
   "source": [
    "Necessary steps:\n",
    "- create output folder\n",
    "- save merged dataset to a csv within the output folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "peripheral-young",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('output',exist_ok=True) #create output folder\n",
    "sp500_with_acct.to_csv('output/sp500_accting_plus_textrisks.csv',index=False) #take merged df, remove index, put in csv "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
