{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the analysis\n",
    "\n",
    "## Getting the data on the sample firms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from time import sleep\n",
    "import urllib.request    \n",
    "from requests_html import HTMLSession # HTMLSession is how requests_html loads requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# open the wiki page with s&p 500 firms\n",
    "session = HTMLSession()\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "r = session.get(url)\n",
    "\n",
    "# where to put the sample firm data and the text files\n",
    "os.makedirs('inputs',exist_ok=True)\n",
    "os.makedirs('text_files',exist_ok=True)\n",
    "os.makedirs('10k_files',exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get the URL to the S&P 500 firms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "table    = r.html.find('#constituents')[0]\n",
    "rows     = table.find('tr')[1:]\n",
    "wikiurls = []\n",
    "securls  = []\n",
    "for row in rows:\n",
    "    a_url = list(row.find('td')[1].absolute_links)[0]\n",
    "    wikiurls.append(a_url)\n",
    "    \n",
    "    a_url = list(row.find('td')[2].absolute_links)[0]\n",
    "    securls.append(a_url)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the wiki sp500 table (with the URL!)    \n",
    "(\n",
    "    pd.read_html(url)[0]\n",
    "    .assign(url = wikiurls, sec_url = securls)   \n",
    "    .to_csv('inputs/sp500_with_url.csv',index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download their wiki pages..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_wiki(get_url,put_here):\n",
    "    if not os.path.exists(put_here): \n",
    "        try:                        \n",
    "            urllib.request.urlretrieve(get_url, put_here) # LUKE SAVED US\n",
    "        except:\n",
    "            print('DOWNLOAD FAILED ON: ',get_url)\n",
    "        else:\n",
    "            sleep(1) # be nice to server  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOAD FAILED ON:  https://en.wikipedia.org/w/index.php?title=Pool_Corporation&action=edit&redlink=1\n"
     ]
    }
   ],
   "source": [
    "# loop over the sample firms, and download the wiki page\n",
    "\n",
    "sp500 = pd.read_csv('inputs/sp500_with_url.csv')\n",
    "\n",
    "for index, row in sp500.iterrows():    \n",
    "    download_wiki(row['url'], \n",
    "            'text_files/' + row['Symbol'] + '.html') # cat and the MBAs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download their 10-K closest to, but before 2/1/20\n",
    "\n",
    "---\n",
    "\n",
    "    ## User Notes: \n",
    "\n",
    "    The only change is that we need to give our function a different URL to download, and we need to find it. My plan: Just have python do the steps I'd do!\n",
    "\n",
    "    What I'd do by hand:\n",
    "    1. Click on the SEC reports link on the wiki page (which we have as the \"sec_url\" variable).\n",
    "    1. Filter to 10-Ks and filings before covid started. (When you do that yourself, the URL just changes to include `&type=10-k&dateb=20200201&count=1` at the end, so I'll add this to the `sec_url`.)\n",
    "    1. Click on the first link on the top table that this loads.\n",
    "    1. Click on the first link on the top table that this loads.\n",
    "\n",
    "    What I'll have python do\n",
    "    1. Open the \"sec_url\" with `requests_html`, with `&type=10-k&dateb=20200201&count=1` at the end.\n",
    "    1. Find the first link on the top table that this loads. I'll use the \"inspect\" trick to find an identifier of some kind for the link. \n",
    "    1. Open that link with `requests_html`. \n",
    "    1. Find the first link on the top table that this loads. I'll use the \"inspect\" trick again, but this time, I'll just find the table, then grab the first row and second column like we did in \"Step #1\" above to grab the sec_url. \n",
    "\n",
    "    _Note: It turns out that a few of the links on Wiki to the firm's filings are not pointing to the correct page (e.g. missing a character in the symbol, or pointing at a portion of the corporation that is no longer the parent). I made no **manual** corrections to this, but in a professional-grade analysis, I would make sure every single ticker was correct so that I could download SEC filing information on every firm. Also, I would have to make a choice about how to deal with Alphabet (there are two versions of it in the sample - should both be kept?)._ \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_10k(get_url,put_here):\n",
    "    if not os.path.exists(put_here): \n",
    "        try:            \n",
    "            # open the firm's sec filing listings\n",
    "            r = session.get(get_url) \n",
    "            \n",
    "            # the links to filings are id=documentsbutton, and the very first is the latest 10k\n",
    "            filingdetail_url = list(r.html.find('#documentsbutton')[0].absolute_links)[0] \n",
    "            \n",
    "            # open that link, now r is the latest 10k's landing page\n",
    "            r = session.get(filingdetail_url) \n",
    "            \n",
    "            # the first class=tableFile is the top table, go to the second row,\n",
    "            # then the third col, and grab the first link within \n",
    "            tenK_url = list(r.html.find('.tableFile')[0].find('tr')[1].find('td')[2].absolute_links)[0] \n",
    "            \n",
    "            # sometimes the default is the XBRL, I just want the html. a little change to the \n",
    "            # url works whenever this happens\n",
    "            tenK_url = tenK_url.replace(\"ix?doc=/\",\"\") \n",
    "            \n",
    "            # DL it\n",
    "            urllib.request.urlretrieve(tenK_url, put_here) # LUKE SAVED US\n",
    "        except:\n",
    "            print('DOWNLOAD FAILED ON: ',get_url)\n",
    "        else:\n",
    "            sleep(1) # be nice to server              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████▍                                                                     | 66/505 [00:00<00:00, 656.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOAD FAILED ON:  https://www.sec.gov/cgi-bin/browse-edgar?CIK=APA&action=getcompany&type=10-k&dateb=20200201&count=1\n",
      "DOWNLOAD FAILED ON:  https://www.sec.gov/cgi-bin/browse-edgar?CIK=BRKB&action=getcompany&type=10-k&dateb=20200201&count=1\n",
      "DOWNLOAD FAILED ON:  https://www.sec.gov/cgi-bin/browse-edgar?CIK=BFB&action=getcompany&type=10-k&dateb=20200201&count=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████                                                           | 128/505 [00:00<00:00, 618.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOAD FAILED ON:  https://www.sec.gov/cgi-bin/browse-edgar?CIK=CARR&action=getcompany&type=10-k&dateb=20200201&count=1\n",
      "DOWNLOAD FAILED ON:  https://www.sec.gov/cgi-bin/browse-edgar?CIK=CTVA&action=getcompany&type=10-k&dateb=20200201&count=1\n",
      "DOWNLOAD FAILED ON:  https://www.sec.gov/cgi-bin/browse-edgar?CIK=DOW&action=getcompany&type=10-k&dateb=20200201&count=1\n",
      "DOWNLOAD FAILED ON:  https://www.sec.gov/cgi-bin/browse-edgar?CIK=FRC&action=getcompany&type=10-k&dateb=20200201&count=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████▉                        | 351/505 [00:00<00:00, 515.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOAD FAILED ON:  https://www.sec.gov/cgi-bin/browse-edgar?CIK=NXPI&action=getcompany&type=10-k&dateb=20200201&count=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 505/505 [00:01<00:00, 367.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOAD FAILED ON:  https://www.sec.gov/cgi-bin/browse-edgar?CIK=OTIS&action=getcompany&type=10-k&dateb=20200201&count=1\n",
      "DOWNLOAD FAILED ON:  https://www.sec.gov/cgi-bin/browse-edgar?CIK=VTRS&action=getcompany&type=10-k&dateb=20200201&count=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sp500 = pd.read_csv('inputs/sp500_with_url.csv')\n",
    "\n",
    "for index, row in tqdm(sp500.iterrows(), total=len(sp500)  ):    \n",
    "    download_10k(get_url = row['sec_url']+'&type=10-k&dateb=20200201&count=1', \n",
    "                put_here = '10k_files/' + row['Symbol'] + '.html') # cat and the MBAs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
