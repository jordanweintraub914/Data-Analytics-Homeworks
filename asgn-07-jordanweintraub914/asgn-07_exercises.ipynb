{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Practice\n",
    "\n",
    "Objective: After this assignment, you can build a pipeline that\n",
    "1. Preprocesses realistic data (multiple variable types) in a pipeline that handles each variable type\n",
    "1. Estimates a model using CV\n",
    "1. Hypertunes a model on a CV folds within training sample\n",
    "1. Finally, evaluate its performance in the test sample\n",
    "\n",
    "Let's start by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data and split off X and y\n",
    "housing = pd.read_csv('input_data2/housing_train.csv')\n",
    "y = np.log(housing.v_SalePrice)\n",
    "housing = housing.drop('v_SalePrice',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure you can be graded accurately, we need to make the \"randomness\" predictable. (I.e. you should get the exact same answers every single time we run this.)\n",
    "\n",
    "Per the recommendations in the [sk-learn documentation](https://scikit-learn.org/stable/common_pitfalls.html#general-recommendations), what that means is we need to put `random_state=rng` inside every function in this file that accepts \"random_state\" as an argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test set for use later - notice the (random_state=rng)\n",
    "rng = np.random.RandomState(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing, y, random_state=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Preprocessing the data\n",
    "\n",
    "1. Set up a single pipeline called `preproc_pipe` to preprocess the data.\n",
    "    1. Impute missing values with SimpleImputer and scale them with StandardScaler\n",
    "    1. `v_Lot_Config`: Use OneHotEncoder on it \n",
    "    1. Drop any other variables (handle this **inside** the pipeline)\n",
    "1. Use this pipeline to preprocess X_train. Describe the resulting data **with two digits.**\n",
    "    - TIP: Use `df_after_transform` from the community codebook to make this easier.\n",
    "    - The a few rows of my print out look like this:\n",
    "    \n",
    "    | | count | mean | std | min  | 25%  | 50% |  75% |  max\n",
    "    | --- | --- | --- | ---  | ---  | --- |  --- |  --- |  ---\n",
    "    |  v_MS_SubClass | 1455 | 0 | 1 | -0.89 | -0.89 | -0.2 | 0.26 | 3.03\n",
    "    |  v_Lot_Frontage | 1455 | 0 | 1 | -2.2 | -0.43 | 0 | 0.39 | 11.07\n",
    "    |  v_Lot_Area  | 1455 | 0 | 1 | -1.17 | -0.39 | -0.11 | 0.19 | 20.68\n",
    "    | v_Overall_Qual | 1455 | 0 | 1 | -3.7 | -0.81 | -0.09 | 0.64 | 2.8\n",
    "\n",
    "1. How many columns are in this object?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Estimating one model\n",
    "\n",
    "1. Report the mean test score (**show 5 digits**) when you use cross validation on a Lasso Model with\n",
    "    - alpha = 0.3, \n",
    "    - CV uses 10 `KFold`s\n",
    "    - R$^2$ scoring \n",
    "1. Now get the optimal alpha (**show 5 digits**) for the lasso model \n",
    "    1. How many of the variables did it select?\n",
    "    3. Report the 5 highest  _non-zero_ coefficients\n",
    "    4. Report the 5 lowest _non-zero_ coefficients \n",
    "    5. Repeat the cross validation above, but use your optimal alpha. What is the mean test score now?\n",
    "    5. To this point, you've done everything on the training test. Now use your predicted coefficients on the test set! Whats the R2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Optimizing and estimating your own models\n",
    "\n",
    "You can walk! Let's try to run! The next skill level is trying more models and picking your favorite. \n",
    "\n",
    "1. Build a pipeline with these 3 steps and **print it to screen** \n",
    "    1. step 1: preprocessing: this includes imputation, scaling numerics, outlier handling, encoding categoricals, and feature creation (polynomial transformations / interactions) \n",
    "    1. step 2: feature \"selection\": Either selectKbest, RFEcv, or PCA\n",
    "    1. step 3: model estimation: [e.g. regression](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)\n",
    "1. Set up a gridsearch to optimize at least 1 hyperparameter in the preprocessing step and at least 1 hyperparameter in the feature selection step, and try at least 50 combinations of parameters in your grid search. **Print your search grid to screen.**\n",
    "1. Output the optimal values from your hyperparameters, fit the best version on the training data (X_train and y_train), then predict y in the test data (using X_test) and tell us how the testing predictions scored for your model!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
