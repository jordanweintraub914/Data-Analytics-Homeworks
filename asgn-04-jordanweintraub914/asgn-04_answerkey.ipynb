{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit your review here \n",
    "\n",
    "https://forms.gle/75paKGVwGhu9y6Pi9\n",
    "\n",
    "Remember, there are usually MANY ways to achieve things programatically. We will focus the reviews on substance and content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Merging\n",
    "\n",
    "Related text: https://ledatascifi.github.io/ledatascifi-2021/content/03/05b_merging.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_datareader as pdr\n",
    "import seaborn as sns\n",
    "# import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1\n",
    "\n",
    "Insert cell(s) below this one as needed to finish this Part.\n",
    "\n",
    "Load the following two datasets and answer these questions. Assume that the French data is the \"left\" dataset and the stock data is the \"right\" dataset. \n",
    "\n",
    "1. How many observations are there in `ff` data?\n",
    "1. How many observations are there in `crsp` data?\n",
    "4. After an inner merge?\n",
    "1. How many observations are there after a left merge?\n",
    "2. After a right merge? \n",
    "3. After an outer merge? \n",
    "1. Why isn't the answer to Q4 and Q5 the same?\n",
    "1. Is this a 1:1, 1:M, M:1, or M:M merge?\n",
    "\n",
    "Remember: Specify `how`, `on`, `indicator=True`, and `validate` on each merge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = pdr.get_data_famafrench('F-F_Research_Data_5_Factors_2x3_daily',start=1980,end=2010)[0] # the [0] is because the imported obect is a dictionary, and key=0 is the dataframe\n",
    "ff = ff.reset_index().rename(columns={\"Mkt-RF\":\"mkt_excess\", \"Date\":\"date\"})\n",
    "crsp = pd.read_stata('https://github.com/LeDataSciFi/ledatascifi-2021/blob/main/data/3firm_ret_1990_2020.dta?raw=true')\n",
    "crsp['ret'] = crsp['ret']*100 # convert to precentage to match FF's convention on scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My answers\n",
    "\n",
    "I use the `merge_type` function from https://ledatascifi.github.io/ledatascifi-2021/content/03/05b_merging.html\n",
    "\n",
    "And simply to avoid the copy-pasting the merge function over and over again, I use a lambda function. You can easily solve this without a lambda function, but it made my code simpler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q1: N in L (ff):           7571\n",
      "Q2: N in R (crsp):         30236\n",
      "Q3: N after Inner merge:   20172\n",
      "Q4: N after L merge:       22700\n",
      "Q5: N after R merge:       30236\n",
      "Q6: N after Outer merge:   32764\n",
      "Q7: Q5 != Q4  b/c because the set of keys in L and R differ.\n",
      "Q8: This merge is a:       one_to_many\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quick_merge = lambda how: pd.merge(left=ff,right=crsp,on='date',how=how,indicator=True,validate=\"one_to_many\")\n",
    "#quick_merge('inner') is equiv to pd.merge(left=ff,right=crsp,on='date',how='inner',indicator=True,validate=\"one_to_many\")\n",
    "\n",
    "def merge_type(df1,df2,on):\n",
    "    # if there are duplicates, dropping them will shrink the key vector\n",
    "    if len(df1[on]) > len(df1[on].drop_duplicates()):\n",
    "        _l = \"many\"\n",
    "    else:\n",
    "        _l = \"one\"\n",
    "    if len(df2[on]) > len(df2[on].drop_duplicates()):\n",
    "        _r = \"many\"\n",
    "    else:\n",
    "        _r = \"one\"\n",
    "    return \"{}_to_{}\".format(_l,_r)\n",
    "\n",
    "########################\n",
    "# answers\n",
    "########################\n",
    "\n",
    "print(\n",
    "f'''\n",
    "Q1: N in L (ff):           {len(ff)}\n",
    "Q2: N in R (crsp):         {len(crsp)}\n",
    "Q3: N after Inner merge:   {len(quick_merge(\"inner\"))}\n",
    "Q4: N after L merge:       {len(quick_merge(\"left\" ))}\n",
    "Q5: N after R merge:       {len(quick_merge(\"right\"))}\n",
    "Q6: N after Outer merge:   {len(quick_merge(\"outer\"))}\n",
    "Q7: Q5 != Q4  b/c because the set of keys in L and R differ.\n",
    "Q8: This merge is a:       {merge_type(ff,crsp,'date')}\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Creating variables around a merge \n",
    "\n",
    "Q9: Suppose you're analyzing the stock returns in the CRSP dataset. Add the FF variables to it. Then add a variable dataset that equals \"the variance of the market return for the year\" for each firm year. (The sentences are enough to pick which \"how\" option to choose.)  `describe()` the crsp dataset after you add the new variables - but you only need to describe `'ret','mkt_excess', 'SMB', 'HML'` and the new variance variable.\n",
    "\n",
    "Note: You don't need to \"annualize\" the variance because the resulting data is daily.\n",
    "\n",
    "HINT: If you get a mean of 1.334374 for the variance variable, that is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My answer: The main point of this question is the following rule\n",
    "\n",
    "Annual volatility of daily market returns must be calculated from a dataset with 1 observation per day. If you try to calculate volatility after merging it to the CRSP data, every day will be repeated 3 times (once per firm) and this will alter your calculations. \n",
    "\n",
    "**So, the general rule: Create variables on the \"lowest level\" datasets possible, and then merge up.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_2104537d_7b75_11eb_9db2_50e085815168\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >mkt_excess</th>    </tr>    <tr>        <th class=\"index_name level0\" >year</th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_2104537d_7b75_11eb_9db2_50e085815168level0_row0\" class=\"row_heading level0 row0\" >2000</th>\n",
       "                        <td id=\"T_2104537d_7b75_11eb_9db2_50e085815168row0_col0\" class=\"data row0 col0\" >2.417</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2104537d_7b75_11eb_9db2_50e085815168level0_row1\" class=\"row_heading level0 row1\" >2001</th>\n",
       "                        <td id=\"T_2104537d_7b75_11eb_9db2_50e085815168row1_col0\" class=\"data row1 col0\" >1.976</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2104537d_7b75_11eb_9db2_50e085815168level0_row2\" class=\"row_heading level0 row2\" >2002</th>\n",
       "                        <td id=\"T_2104537d_7b75_11eb_9db2_50e085815168row2_col0\" class=\"data row2 col0\" >2.527</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2104537d_7b75_11eb_9db2_50e085815168level0_row3\" class=\"row_heading level0 row3\" >2003</th>\n",
       "                        <td id=\"T_2104537d_7b75_11eb_9db2_50e085815168row3_col0\" class=\"data row3 col0\" >1.102</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2104537d_7b75_11eb_9db2_50e085815168level0_row4\" class=\"row_heading level0 row4\" >2004</th>\n",
       "                        <td id=\"T_2104537d_7b75_11eb_9db2_50e085815168row4_col0\" class=\"data row4 col0\" >0.518</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2104537d_7b75_11eb_9db2_50e085815168level0_row5\" class=\"row_heading level0 row5\" >2005</th>\n",
       "                        <td id=\"T_2104537d_7b75_11eb_9db2_50e085815168row5_col0\" class=\"data row5 col0\" >0.440</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2104537d_7b75_11eb_9db2_50e085815168level0_row6\" class=\"row_heading level0 row6\" >2006</th>\n",
       "                        <td id=\"T_2104537d_7b75_11eb_9db2_50e085815168row6_col0\" class=\"data row6 col0\" >0.449</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2104537d_7b75_11eb_9db2_50e085815168level0_row7\" class=\"row_heading level0 row7\" >2007</th>\n",
       "                        <td id=\"T_2104537d_7b75_11eb_9db2_50e085815168row7_col0\" class=\"data row7 col0\" >0.985</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2104537d_7b75_11eb_9db2_50e085815168level0_row8\" class=\"row_heading level0 row8\" >2008</th>\n",
       "                        <td id=\"T_2104537d_7b75_11eb_9db2_50e085815168row8_col0\" class=\"data row8 col0\" >6.344</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2104537d_7b75_11eb_9db2_50e085815168level0_row9\" class=\"row_heading level0 row9\" >2009</th>\n",
       "                        <td id=\"T_2104537d_7b75_11eb_9db2_50e085815168row9_col0\" class=\"data row9 col0\" >2.959</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26a55a8f580>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the year (as a variable) in order compute the var for a year\n",
    "ff['year'] = ff['date'].dt.year\n",
    "\n",
    "# just so we can verify if we are right, let's figure out the within-year var\n",
    "# compute variance within each year. 30 years of it:\n",
    "display (ff.groupby('year')                      # for each year\n",
    "        ['mkt_excess'].var()                     # compute variance\n",
    "        [-10:].to_frame().style.format('{:.3f}') # unnecssary: just to print nicer\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I KNOW that all 2009 returns should be matched to a variance of 2.959! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >ret</th>        <th class=\"col_heading level0 col1\" >mkt_excess</th>        <th class=\"col_heading level0 col2\" >SMB</th>        <th class=\"col_heading level0 col3\" >HML</th>        <th class=\"col_heading level0 col4\" >varExcess</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row0_col0\" class=\"data row0 col0\" >30,236.000</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row0_col1\" class=\"data row0 col1\" >20,172.000</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row0_col2\" class=\"data row0 col2\" >20,172.000</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row0_col3\" class=\"data row0 col3\" >20,172.000</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row0_col4\" class=\"data row0 col4\" >20,172.000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row1_col0\" class=\"data row1 col0\" >0.075</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row1_col1\" class=\"data row1 col1\" >0.024</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row1_col2\" class=\"data row1 col2\" >0.007</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row1_col3\" class=\"data row1 col3\" >0.017</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row1_col4\" class=\"data row1 col4\" >1.338</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row2_col0\" class=\"data row2 col0\" >2.222</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row2_col1\" class=\"data row2 col1\" >1.157</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row2_col2\" class=\"data row2 col2\" >0.587</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row2_col3\" class=\"data row2 col3\" >0.643</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row2_col4\" class=\"data row2 col4\" >1.397</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row3_col0\" class=\"data row3 col0\" >-51.869</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row3_col1\" class=\"data row3 col1\" >-8.950</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row3_col2\" class=\"data row3 col2\" >-4.180</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row3_col3\" class=\"data row3 col3\" >-4.390</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row3_col4\" class=\"data row3 col4\" >0.232</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row4_col0\" class=\"data row4 col0\" >-0.981</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row4_col1\" class=\"data row4 col1\" >-0.480</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row4_col2\" class=\"data row4 col2\" >-0.330</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row4_col3\" class=\"data row4 col3\" >-0.260</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row4_col4\" class=\"data row4 col4\" >0.440</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row5_col0\" class=\"data row5 col0\" >0.000</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row5_col1\" class=\"data row5 col1\" >0.060</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row5_col2\" class=\"data row5 col2\" >0.020</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row5_col3\" class=\"data row5 col3\" >0.000</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row5_col4\" class=\"data row5 col4\" >0.882</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row6_col0\" class=\"data row6 col0\" >1.074</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row6_col1\" class=\"data row6 col1\" >0.550</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row6_col2\" class=\"data row6 col2\" >0.340</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row6_col3\" class=\"data row6 col3\" >0.270</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row6_col4\" class=\"data row6 col4\" >1.546</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row7_col0\" class=\"data row7 col0\" >33.228</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row7_col1\" class=\"data row7 col1\" >11.350</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row7_col2\" class=\"data row7 col2\" >4.490</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row7_col3\" class=\"data row7 col3\" >4.830</td>\n",
       "                        <td id=\"T_210da3fe_7b75_11eb_8c7a_50e085815168row7_col4\" class=\"data row7 col4\" >6.344</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26a53a1af70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save annual vol, then do the FF-CRSP merge, then merge in annual vol\n",
    "ann_vol = ff.groupby('year')['mkt_excess'].var().reset_index()\n",
    "ann_vol.columns = ['year','varExcess']\n",
    "\n",
    "stock_analysis_df = pd.merge(crsp, ff, on='date',\n",
    "                             how='left',validate='many_to_one')   # merge FF-CRSP\n",
    "stock_analysis_df = pd.merge(stock_analysis_df, ann_vol, on='year',\n",
    "                             how='left',validate='many_to_one')   # add annual vol\n",
    "display(stock_analysis_df[['ret','mkt_excess', 'SMB', 'HML','varExcess']]\n",
    "        .describe().style.format('{:,.3f}')) # prettier output, not necessary\n",
    "\n",
    "# same output, but uses chains so no intermediate objects are named\n",
    "# (crsp\n",
    "#  .merge(ff,     on='date',how='left',validate='many_to_one')\n",
    "#  .merge(ann_vol,on='year',how='left',validate='many_to_one')\n",
    "#  [['ret','mkt_excess', 'SMB', 'HML','varExcess']].describe()  \n",
    "#  .style.format('{:,.3f}')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See! The 2009 values are wrong! Look:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4791    2.950369\n",
       "4792    2.950369\n",
       "4793    2.950369\n",
       "4794    2.950369\n",
       "4795    2.950369\n",
       "Name: wrongVar, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# route 3 WRONG: calculate variance AFTER the merge\n",
    "temp = pd.merge(left = crsp, right = ff,on='date',how='left', validate='many_to_one')\n",
    "temp['year'] = temp['date'].dt.year\n",
    "# the \"transform\" method does an operation on groups (after a groupby)\n",
    "# but returns a dataset of the SAME size as before (rather than shrinking the dataset)\n",
    "temp['wrongVar'] = temp.groupby('year')['mkt_excess'].transform(lambda x: x.var())\n",
    "temp[['ret','mkt_excess', 'SMB', 'HML','wrongVar']].describe()\n",
    "\n",
    "print('See! The 2009 values are wrong! Look:')\n",
    "temp.query('year == 2009')['wrongVar'].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Adding a new variable via merge\n",
    "\n",
    "Insert cell(s) below this one as needed to finish this Part.\n",
    "\n",
    "Imagine you're analyzing our CCM dataset. (The code to download the CCM data is on 3.2.5 of the website.) Add a variable call \"PatentStock\" from [here](https://github.com/LeDataSciFi/ledatascifi-2021/blob/main/data/two_pat_vars.csv) to it. (The sentences are enough to pick which \"how\" option to choose in the merge.)\n",
    "\n",
    "Q10: `.describe()` the \"l_a\" and the \"PatentStock\" variables after the merge. \n",
    "Q11: Tabulate the _merge variable. \n",
    "\n",
    "HINT: As you've seen several times before, pandas can download a csv file with just a URL. The \"hitch\" here is that I'm not giving you the URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/LeDataSciFi/ledatascifi-2021/blob/main/data/two_pat_vars.csv?raw=true'\n",
    "patent = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = 'https://github.com/LeDataSciFi/ledatascifi-2021/blob/main/data/CCM_cleaned_for_class.zip?raw=true'\n",
    "\n",
    "#firms = pd.read_stata(url)   \n",
    "# <-- that code would work, but GH said it was too big and\n",
    "# forced me to zip it, so here is the work around to download it:\n",
    "\n",
    "with urlopen(url) as request:\n",
    "    data = BytesIO(request.read())\n",
    "\n",
    "with ZipFile(data) as archive:\n",
    "    with archive.open(archive.namelist()[0]) as stata:\n",
    "        ccm = pd.read_stata(stata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l_a</th>\n",
       "      <th>prodmktfluid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>222978.000000</td>\n",
       "      <td>88332.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.226964</td>\n",
       "      <td>7.479115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.408634</td>\n",
       "      <td>3.909412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-6.907755</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.476993</td>\n",
       "      <td>4.626448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.094452</td>\n",
       "      <td>6.788517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.864064</td>\n",
       "      <td>9.634171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.142903</td>\n",
       "      <td>40.775070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 l_a  prodmktfluid\n",
       "count  222978.000000  88332.000000\n",
       "mean        5.226964      7.479115\n",
       "std         2.408634      3.909412\n",
       "min        -6.907755      0.000000\n",
       "25%         3.476993      4.626448\n",
       "50%         5.094452      6.788517\n",
       "75%         6.864064      9.634171\n",
       "max        15.142903     40.775070"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both          172315\n",
      "left_only      50686\n",
      "right_only         0\n",
      "Name: _merge, dtype: int64\n",
      "223001\n"
     ]
    }
   ],
   "source": [
    "print(len(ccm))\n",
    "ccm_with_pats = pd.merge(ccm,patent,how='left',\n",
    "         on=['gvkey','fyear'],\n",
    "         validate='one_to_one',\n",
    "         indicator=True\n",
    "        )\n",
    "display(ccm_with_pats[['l_a','prodmktfluid']].describe())\n",
    "print(ccm_with_pats['_merge'].value_counts())\n",
    "print(len(ccm_with_pats))\n",
    "\n",
    "# fluidity.columns # year, gvkey\n",
    "# ccm.columns  # gvkey, fyear"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
